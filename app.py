import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score
import xgboost as xgb
import lightgbm as lgb
import shap

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams["font.family"] = ["SimHei", "WenQuanYi Micro Hei", "Heiti TC"]
plt.rcParams["axes.unicode_minus"] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="å†æ‰‹æœ¯é£é™©é¢„æµ‹æ¨¡å‹",
    page_icon="âš•ï¸",
    layout="wide"
)

# åŠ è½½æ•°æ®
@st.cache_data
def load_data():
    try:
        df = pd.read_excel("data/2222222.xlsx")
        return df
    except Exception as e:
        st.error(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
        st.stop()

# è®­ç»ƒæ¨¡å‹
@st.cache_data
def train_models(df):
    # æå–ç‰¹å¾å’Œç›®æ ‡å˜é‡
    X = df.drop("Unplanned reoperation", axis=1)
    y = df["Unplanned reoperation"]
    
    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    
    # å®šä¹‰æ¨¡å‹
    models = {
        "é€»è¾‘å›å½’ (LR)": LogisticRegression(max_iter=1000, random_state=42),
        "XGBoost": xgb.XGBClassifier(random_state=42),
        "LightGBM": lgb.LGBMClassifier(random_state=42),
        "éšæœºæ£®æ— (RF)": RandomForestClassifier(random_state=42),
        "æ”¯æŒå‘é‡æœº (SVM)": SVC(probability=True, random_state=42)
    }
    
    # è®­ç»ƒå¹¶è¯„ä¼°æ¨¡å‹
    results = {}
    trained_models = {}
    
    for name, model in models.items():
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        
        accuracy = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred)
        
        results[name] = {
            "å‡†ç¡®ç‡": accuracy,
            "F1åˆ†æ•°": f1,
            "å¬å›ç‡": recall,
            "ç²¾ç¡®ç‡": precision
        }
        
        trained_models[name] = model
    
    # é€‰æ‹©æœ€ä½³æ¨¡å‹ (åŸºäºF1åˆ†æ•°)
    best_model_name = max(results, key=lambda x: results[x]["F1åˆ†æ•°"])
    best_model = trained_models[best_model_name]
    
    return X_test, results, best_model, best_model_name, X.columns

# ä¸»åº”ç”¨
def main():
    st.title("â¤ï¸ å†æ‰‹æœ¯é£é™©é¢„æµ‹æ¨¡å‹")
    st.markdown("æœ¬åº”ç”¨ä½¿ç”¨æœºå™¨å­¦ä¹ ç®—æ³•é¢„æµ‹æœ¯åå†æ‰‹æœ¯é£é™©ï¼Œå¸®åŠ©åŒ»ç”Ÿåšå‡ºæ›´æ˜æ™ºçš„å†³ç­–ã€‚")
    
    # åŠ è½½æ•°æ®
    df = load_data()
    
    # æ˜¾ç¤ºæ•°æ®æ¦‚è§ˆ
    with st.expander("æ•°æ®æ¦‚è§ˆ"):
        st.write(f"æ•°æ®é›†åŒ…å« {df.shape[0]} æ¡è®°å½•å’Œ {df.shape[1]} ä¸ªç‰¹å¾ã€‚")
        st.dataframe(df.head())
    
    # è®­ç»ƒæ¨¡å‹
    X_test, results, best_model, best_model_name, feature_names = train_models(df)
    
    # æ˜¾ç¤ºæ¨¡å‹æ€§èƒ½
    st.subheader("æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ")
    performance_df = pd.DataFrame(results).T
    st.dataframe(performance_df.style.highlight_max(axis=0))
    
    st.write(f"åŸºäº F1 åˆ†æ•°ï¼Œ**{best_model_name}** è¡¨ç°æœ€ä½³ã€‚")
    
    # SHAP è§£é‡Š
    st.subheader("ç‰¹å¾é‡è¦æ€§åˆ†æ")
    st.markdown("ä½¿ç”¨ SHAP (SHapley Additive exPlanations) æ–¹æ³•è§£é‡Šæ¨¡å‹é¢„æµ‹ï¼š")
    
    try:
        # è®¡ç®— SHAP å€¼
        explainer = shap.Explainer(best_model)
        shap_values = explainer(X_test)
        
        # æ˜¾ç¤º SHAP æ±‡æ€»å›¾
        fig, ax = plt.subplots(figsize=(10, 6))
        shap.summary_plot(shap_values, X_test, plot_type="bar", show=False)
        plt.tight_layout()
        st.pyplot(fig)
        
        # æ˜¾ç¤ºç‰¹å¾é‡è¦æ€§è¡¨æ ¼
        feature_importance = pd.Series(
            np.abs(shap_values.values).mean(axis=0), 
            index=feature_names
        ).sort_values(ascending=False)
        
        st.write("æŒ‰é‡è¦æ€§æ’åºçš„ç‰¹å¾ï¼š")
        st.dataframe(feature_importance.reset_index().rename(
            columns={"index": "ç‰¹å¾", 0: "å¹³å‡SHAPå€¼"}
        ))
        
    except Exception as e:
        st.warning(f"SHAP åˆ†æå¤±è´¥: {e}")
        st.write("å¯èƒ½æ˜¯ç”±äºæ¨¡å‹ç±»å‹ä¸æ”¯æŒSHAPè‡ªåŠ¨è§£é‡Šã€‚")
    
    # ç”¨æˆ·é¢„æµ‹ç•Œé¢
    st.subheader("ğŸ” æ‚£è€…å†æ‰‹æœ¯é£é™©é¢„æµ‹")
    
    with st.form("prediction_form"):
        st.markdown("### è¾“å…¥æ‚£è€…ç‰¹å¾")
        
        # åˆ›å»ºè¾“å…¥è¡¨å•
        input_data = {}
        for feature in feature_names:
            min_val = float(df[feature].min())
            max_val = float(df[feature].max())
            mean_val = float(df[feature].mean())
            
            # æ ¹æ®ç‰¹å¾ç±»å‹è°ƒæ•´è¾“å…¥æ§ä»¶
            if len(df[feature].unique()) <= 3:  # åˆ†ç±»ç‰¹å¾
                input_data[feature] = st.select_slider(
                    feature,
                    options=sorted(df[feature].unique().tolist()),
                    value=mean_val if mean_val in df[feature].unique() else min_val
                )
            else:  # è¿ç»­ç‰¹å¾
                input_data[feature] = st.slider(
                    feature,
                    min_value=min_val,
                    max_value=max_val,
                    value=mean_val,
                    step=(max_val - min_val) / 20
                )
        
        # æäº¤é¢„æµ‹
        submitted = st.form_submit_button("é¢„æµ‹é£é™©")
        
        if submitted:
            # åˆ›å»ºé¢„æµ‹æ•°æ®
            input_df = pd.DataFrame([input_data])
            
            # é¢„æµ‹
            prediction = best_model.predict(input_df)[0]
            if hasattr(best_model, "predict_proba"):
                proba = best_model.predict_proba(input_df)[0][1]
            else:
                proba = None
            
            # æ˜¾ç¤ºé¢„æµ‹ç»“æœ
            st.subheader("ğŸ“Š é¢„æµ‹ç»“æœ")
            
            if prediction == 1:
                st.error(f"é¢„æµ‹ç»“æœï¼š**éœ€è¦å†æ‰‹æœ¯**")
                if proba is not None:
                    st.warning(f"å†æ‰‹æœ¯é£é™©æ¦‚ç‡ï¼š{proba:.2%}")
            else:
                st.success(f"é¢„æµ‹ç»“æœï¼š**æ— éœ€å†æ‰‹æœ¯**")
                if proba is not None:
                    st.info(f"å†æ‰‹æœ¯é£é™©æ¦‚ç‡ï¼š{proba:.2%}")
            
            # å°è¯•æ˜¾ç¤ºå±€éƒ¨SHAPè§£é‡Š
            try:
                if 'explainer' in locals():
                    st.subheader("ğŸ” é¢„æµ‹è§£é‡Š")
                    local_shap_values = explainer(input_df)
                    
                    fig, ax = plt.subplots(figsize=(10, 6))
                    shap.waterfall_plot(local_shap_values[0], max_display=10, show=False)
                    plt.tight_layout()
                    st.pyplot(fig)
                    
                    st.markdown("ä»¥ä¸Šå›¾è¡¨æ˜¾ç¤ºäº†å„ç‰¹å¾å¯¹æœ¬æ¬¡é¢„æµ‹ç»“æœçš„è´¡çŒ®ç¨‹åº¦ã€‚")
            except Exception as e:
                st.warning(f"æ— æ³•ç”Ÿæˆå±€éƒ¨è§£é‡Š: {e}")

if __name__ == "__main__":
    main()